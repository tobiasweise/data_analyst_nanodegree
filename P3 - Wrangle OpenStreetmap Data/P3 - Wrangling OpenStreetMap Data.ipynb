{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Wrangling OpenStreetMap Data for South-East Berlin (Neukölln) with Python and MongoDB\n",
    "\n",
    "In this project, I wrangle OpenStreetMap Data for the South-East of Berlin, with a special focus on the Berlin-Neukölln area in Germany, my home region. In the following sections, I will present how to convert the OSM XML file to  JSON to import it into MongoDB. Next, I will inspect the data for inconsistencies and clean up some of the data. Third, I will give some descriptives of the data-set to answer some interesting questions about Saxony-Anhalt.\n",
    "\n",
    "## Obtaining OpenStreetMap Data\n",
    "\n",
    "I obtained OSM XML data via the [overpass API](http://overpass-api.de/query_form.html) for the Neukölln area with the following query `(node(52.3706,13.3154,52.5005,13.6642);<;);out meta;`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "\n",
    "First, I need to get an overview of the XML data to be able to later convert it and import it into MongoDB. To get an overview of the tags in the XML, I first iterate over them and list them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'member': 104095,\n",
       " 'meta': 1,\n",
       " 'nd': 1881691,\n",
       " 'node': 1557724,\n",
       " 'note': 1,\n",
       " 'osm': 1,\n",
       " 'relation': 4242,\n",
       " 'tag': 1739250,\n",
       " 'way': 230341}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "infile = 'neukolln.osm'\n",
    "\n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for events, child in ET.iterparse(filename, events=('start',)):\n",
    "        if child.tag not in tags:\n",
    "            tags[child.tag] = 1\n",
    "        else:\n",
    "            tags[child.tag] = tags[child.tag] + 1\n",
    "        child.clear()\n",
    "    return tags\n",
    "\n",
    "count_tags(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As the data extract also includes data for Berlin suburbs other than Neukölln, I need to filter the data to exclude those other cities. Relevant data is included in the tag `addr:suburb` tag. The following code shows which other suburbs are included in the data-set. Otherwise, the data for the suburb field appears to be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adlershof': 2750,\n",
       " 'Alt-Mariendorf': 1,\n",
       " 'Alt-Treptow': 698,\n",
       " 'Altglienicke': 5900,\n",
       " 'Baumschulenweg': 2171,\n",
       " 'Biesdorf': 2756,\n",
       " 'Bohnsdorf': 4353,\n",
       " 'Britz': 5566,\n",
       " 'Buckow': 6354,\n",
       " 'Charlottenburg': 58,\n",
       " 'Friedenau': 1635,\n",
       " 'Friedrichsfelde': 587,\n",
       " 'Friedrichshagen': 2586,\n",
       " 'Friedrichshain': 498,\n",
       " 'Gropiusstadt': 981,\n",
       " 'Grünau': 1162,\n",
       " 'Heinersdorf': 1,\n",
       " 'Johannisthal': 2822,\n",
       " 'Karlshorst': 3862,\n",
       " 'Kaulsdorf': 3065,\n",
       " 'Kreuzberg': 4581,\n",
       " 'Köpenick': 8536,\n",
       " 'Lankwitz': 5469,\n",
       " 'Lichtenberg': 4,\n",
       " 'Lichtenrade': 10288,\n",
       " 'Lichterfelde': 4590,\n",
       " 'Mahlsdorf': 5776,\n",
       " 'Mariendorf': 6169,\n",
       " 'Marienfelde': 3701,\n",
       " 'Müggelheim': 1415,\n",
       " 'Münchehofe': 8,\n",
       " 'Neukölln': 6232,\n",
       " 'Niederschöneweide': 1282,\n",
       " 'Oberschöneweide': 1544,\n",
       " 'Plänterwald': 807,\n",
       " 'Rudow': 10634,\n",
       " 'Rummelsburg': 527,\n",
       " 'Schmöckwitz': 1350,\n",
       " 'Schöneberg': 5949,\n",
       " 'Schöneweide': 1,\n",
       " 'Steglitz': 4744,\n",
       " 'Tempelhof': 5311,\n",
       " 'Tiergarten': 95,\n",
       " 'Vorwerk': 3,\n",
       " 'Waldesruh': 7,\n",
       " 'Waltersdorf': 8,\n",
       " 'Waßmannsdorf': 19,\n",
       " 'Wilmersdorf': 2799}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_suburbs(filename):\n",
    "    suburbs = {}\n",
    "    for events, child in ET.iterparse(filename, events=('start',)):\n",
    "        if (child.tag == \"tag\") and (child.attrib['k'] == \"addr:suburb\"):\n",
    "            if child.attrib['v'] not in suburbs:\n",
    "                suburbs[child.attrib['v']] = 1\n",
    "            else:\n",
    "                suburbs[child.attrib['v']] = suburbs[child.attrib['v']] + 1\n",
    "        child.clear()\n",
    "    return suburbs\n",
    "\n",
    "count_suburbs(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I look for various problems in the data-set to fix them before importing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip-Codes\n",
    "\n",
    "Next to the suburb information, data on postal codes are another important piece of information for selecting only those nodes that relate to Neukölln. The zip range for Neukölln is 10965 - 12099. The data looks fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10965': 1049,\n",
       " '10967': 912,\n",
       " '10969': 270,\n",
       " '10997': 669,\n",
       " '10999': 995,\n",
       " '12043': 763,\n",
       " '12045': 625,\n",
       " '12047': 689,\n",
       " '12049': 801,\n",
       " '12051': 1162,\n",
       " '12053': 757,\n",
       " '12055': 689,\n",
       " '12057': 891,\n",
       " '12059': 761,\n",
       " '12099': 1326}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_zips(filename):\n",
    "    zips = {}\n",
    "    for events, child in ET.iterparse(filename, events=('start',)):\n",
    "        if (child.tag == \"tag\") and (child.attrib['k'] == \"addr:postcode\") and (int(child.attrib['v']) > 10964) and (int(child.attrib['v']) < 12100):\n",
    "            if child.attrib['v'] not in zips:\n",
    "                zips[child.attrib['v']] = 1\n",
    "            else:\n",
    "                zips[child.attrib['v']] = zips[child.attrib['v']] + 1\n",
    "        child.clear()\n",
    "    return zips\n",
    "\n",
    "count_zips(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Street Names\n",
    "\n",
    "Finally, I check for inconsistencies in naming streets. Street name data is contained in the `addr:street` field. Here, I particularly look for the presence of typical abbreviations for `street` and `place`. There are a few issues that need to be addressed for 3 streets that were abbreviated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Graefestr.': 1, 'Rheinstr.': 1, 'Weichselstr.': 1}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_streets(filename):\n",
    "    streets = {}\n",
    "    for events, child in ET.iterparse(filename, events=('start',)):\n",
    "        if (child.tag == \"tag\") and (child.attrib['k'] == \"addr:street\"):\n",
    "            if (\"str.\" in child.attrib['v']) or (\"pl.\" in child.attrib['v']):\n",
    "                if child.attrib['v'] not in streets:\n",
    "                    streets[child.attrib['v']] = 1\n",
    "                else:\n",
    "                    streets[child.attrib['v']] = streets[child.attrib['v']] + 1\n",
    "        child.clear()\n",
    "    return streets\n",
    "\n",
    "count_streets(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting . in fields later used for MongoDB keys\n",
    "\n",
    "MongoDB does not accept periods in key fields. Therefore, I need to see if this is a problem in the data-set. The results below illustrate that this is only a minor problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 'link:berlin.de', 'v': 'http://www.berlin.de/restaurants/2392663-1622830'}\n",
      "{'k': 'step.height', 'v': 'high'}\n",
      "{'k': 'step.condition', 'v': 'even'}\n",
      "{'k': 'step.height', 'v': 'normal'}\n",
      "{'k': 'step.length', 'v': 'normal'}\n"
     ]
    }
   ],
   "source": [
    "for events, child in ET.iterparse(infile, events=('start',)):\n",
    "    if (child.tag == \"tag\"):\n",
    "        for tags in child.iter('tag'):\n",
    "            if '.' in tags.attrib['k']:\n",
    "                print(tags.attrib)\n",
    "    child.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data-Cleansing Function\n",
    "\n",
    "There are two issues:\n",
    "\n",
    "1.  cleaning street names\n",
    "2.  replacing \".\" in the `k` attributes of the `<tag>` elements that will later become keys in MongoDB, which does not allow for `.` in keys.\n",
    "\n",
    "To achieve this goal, I define a cleaning function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleaning_keys(text):\n",
    "    cleaned = text.replace('.', '_')\n",
    "    return cleaned\n",
    "\n",
    "def cleaning_streetname(text):\n",
    "    cleaned = text.replace('str.', 'straße')\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing to MongoDB\n",
    "\n",
    "Next, I import the filtered and cleaned data to a local MongoDB instance. For the import, I will first convert the parsed XML document to JSON format. I only import `node` and `way` information. To save memory usage, I will immediately import the created python dictionary to a local MongoDB instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client.users\n",
    "\n",
    "def ConvertToJSON(filename):\n",
    "    #data = [] #could hold the total list of dictionaries, if needed\n",
    "    for events, child in ET.iterparse(filename, events=('start',)):\n",
    "        if child.tag == \"node\" or child.tag == \"way\":\n",
    "            entry_dict = {} # will hold the entries for each child\n",
    "            tag_dict = {} # will hold attributes of <tag> for nodes and ways\n",
    "            \n",
    "            entry_dict = child.attrib\n",
    "            entry_dict['data_type'] = child.tag\n",
    "            for tags in child.iter(\"tag\"):\n",
    "                if 'addr:street' in tags.attrib['k']: # cleaning street names and adding to dict\n",
    "                    tag_dict[cleaning_keys(tags.attrib['k'])] = cleaning_streetname(tags.attrib['v'])\n",
    "                else: # removing dots from keys and adding to dict\n",
    "                    tag_dict[cleaning_keys(tags.attrib['k'])] = tags.attrib['v']\n",
    "                \n",
    "            entry_dict.update(tag_dict)\n",
    "            #data.append(entry_dict)\n",
    "            db.neukolln.insert_one(entry_dict)\n",
    "        child.clear()\n",
    "    \n",
    "\n",
    "ConvertToJSON(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the data\n",
    "\n",
    "First, here's an overview of the resulting database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('57b23f583afdf0312caf45b7'),\n",
      " 'addr:city': 'Berlin',\n",
      " 'addr:country': 'DE',\n",
      " 'addr:housenumber': '6',\n",
      " 'addr:postcode': '12047',\n",
      " 'addr:street': 'Sonnenallee',\n",
      " 'addr:suburb': 'Neukölln',\n",
      " 'changeset': '34571755',\n",
      " 'data_type': 'node',\n",
      " 'id': '68363380',\n",
      " 'lat': '52.4874054',\n",
      " 'lon': '13.4257380',\n",
      " 'name': 'Days Inn Berlin City South',\n",
      " 'phone': '+4930613820',\n",
      " 'timestamp': '2015-10-11T15:33:17Z',\n",
      " 'tourism': 'hotel',\n",
      " 'uid': '2754647',\n",
      " 'user': 'BenSim78',\n",
      " 'version': '14',\n",
      " 'website': 'http://www.euro-hotel.net',\n",
      " 'wheelchair': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "# This is a sample entry\n",
    "pprint.pprint(db.neukolln.find_one({'addr:suburb': 'Neukölln'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1788065\n"
     ]
    }
   ],
   "source": [
    "# The total number of entries\n",
    "print(db.neukolln.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6120\n"
     ]
    }
   ],
   "source": [
    "# The total number of entries with an Neukölln suburb entry\n",
    "print(db.neukolln.count({'addr:suburb': 'Neukölln'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121860096\n"
     ]
    }
   ],
   "source": [
    "# The total size of the database\n",
    "size = db.command({'collstats': 'neukolln'})\n",
    "print(size['storageSize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1557724\n"
     ]
    }
   ],
   "source": [
    "# Number of nodes\n",
    "print(db.neukolln.count({'data_type': 'node'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230341\n"
     ]
    }
   ],
   "source": [
    "# Number of ways\n",
    "print(db.neukolln.count({'data_type': 'way'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amenities in Neukölln and other suburbs\n",
    "\n",
    "Neukölln, especially its northern parts, is known for its many restaurants, bars and cafes. Let's look at the data, comparing Neukölln to all other suburbs in the data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'pub', 'count': 89}\n",
      "{'_id': 'restaurant', 'count': 58}\n",
      "{'_id': 'cafe', 'count': 45}\n",
      "{'_id': 'fast_food', 'count': 31}\n",
      "{'_id': 'kindergarten', 'count': 28}\n",
      "{'_id': 'bar', 'count': 24}\n",
      "{'_id': 'post_box', 'count': 22}\n",
      "{'_id': 'pharmacy', 'count': 17}\n",
      "{'_id': 'school', 'count': 15}\n",
      "{'_id': 'place_of_worship', 'count': 14}\n"
     ]
    }
   ],
   "source": [
    "# count of amenities in Neukölln\n",
    "query = [\n",
    "        {'$match': {'addr:suburb': 'Neukölln', 'amenity' : {'$exists' : 'true'}}}, \n",
    "        {'$group': {'_id': '$amenity', 'count': {'$sum' : 1}}}, \n",
    "        {'$sort': {'count': -1}},\n",
    "        {'$limit': 10} \n",
    "]\n",
    "\n",
    "for doc in db.neukolln.aggregate(query):\n",
    "    pprint.pprint(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'bench', 'count': 1828}\n",
      "{'_id': 'parking', 'count': 1757}\n",
      "{'_id': 'restaurant', 'count': 1119}\n",
      "{'_id': 'waste_basket', 'count': 923}\n",
      "{'_id': 'kindergarten', 'count': 821}\n",
      "{'_id': 'recycling', 'count': 779}\n",
      "{'_id': 'post_box', 'count': 689}\n",
      "{'_id': 'fast_food', 'count': 663}\n",
      "{'_id': 'cafe', 'count': 596}\n",
      "{'_id': 'bicycle_parking', 'count': 439}\n"
     ]
    }
   ],
   "source": [
    "# count of amenities in other south-eastern Berlin areas\n",
    "query = [\n",
    "        {'$match': {'addr:suburb': {'$ne': 'Neukölln'}, 'amenity' : {'$exists' : 'true'}}}, \n",
    "        {'$group': {'_id': '$amenity', 'count': {'$sum' : 1}}}, \n",
    "        {'$sort': {'count': -1}},\n",
    "        {'$limit': 10} \n",
    "]\n",
    "\n",
    "for doc in db.neukolln.aggregate(query):\n",
    "    pprint.pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that there is indeed a relatively high presence of restaurants and bars and cafés in Neukölln, compared to its more suburban neighbors. \n",
    "\n",
    "Next, let's look at the types of cuisines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries with cuisine 96\n",
      "{'_id': 'italian', 'count': 10}\n",
      "{'_id': 'turkish', 'count': 9}\n",
      "{'_id': 'asian', 'count': 7}\n",
      "{'_id': 'coffee_shop', 'count': 7}\n",
      "{'_id': 'burger', 'count': 6}\n",
      "{'_id': 'indian', 'count': 6}\n",
      "{'_id': 'german', 'count': 5}\n",
      "{'_id': 'pizza', 'count': 5}\n",
      "{'_id': 'greek', 'count': 3}\n",
      "{'_id': 'chinese', 'count': 3}\n"
     ]
    }
   ],
   "source": [
    "# count of cuisines in Neukölln\n",
    "total = db.neukolln.count({'addr:suburb': 'Neukölln', 'cuisine' : {'$exists' : 'true'}})\n",
    "print(\"Total entries with cuisine\", total)\n",
    "\n",
    "query = [\n",
    "        {'$match': {'addr:suburb': 'Neukölln', 'cuisine' : {'$exists' : 'true'}}},\n",
    "        {'$group': {'_id': '$cuisine', 'count': {'$sum' : 1}}}, \n",
    "        {'$sort': {'count': -1}},\n",
    "        {'$limit': 10} \n",
    "]\n",
    "\n",
    "for doc in db.neukolln.aggregate(query):\n",
    "    pprint.pprint(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries with cuisine 1384\n",
      "{'_id': 'italian', 'count': 230}\n",
      "{'_id': 'kebab', 'count': 111}\n",
      "{'_id': 'german', 'count': 99}\n",
      "{'_id': 'asian', 'count': 70}\n",
      "{'_id': 'pizza', 'count': 64}\n",
      "{'_id': 'burger', 'count': 59}\n",
      "{'_id': 'indian', 'count': 57}\n",
      "{'_id': 'turkish', 'count': 56}\n",
      "{'_id': 'ice_cream', 'count': 49}\n",
      "{'_id': 'chinese', 'count': 48}\n"
     ]
    }
   ],
   "source": [
    "# count of cuisines in other south-eastern Berlin areas\n",
    "total = db.neukolln.count({'addr:suburb': {'$ne': 'Neukölln'}, 'cuisine' : {'$exists' : 'true'}})\n",
    "print(\"Total entries with cuisine\", total)\n",
    "query = [\n",
    "        {'$match': {'addr:suburb': {'$ne': 'Neukölln'}, 'cuisine' : {'$exists' : 'true'}}}, \n",
    "        {'$group': {'_id': '$cuisine', 'count': {'$sum' : 1}}}, \n",
    "        {'$sort': {'count': -1}},\n",
    "        {'$limit': 10} \n",
    "]\n",
    "\n",
    "for doc in db.neukolln.aggregate(query):\n",
    "    pprint.pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, Italian food is very popular in the whole region. However, german food is more frequent in the suburbs.\n",
    "\n",
    "### Common Names for amenities\n",
    "\n",
    "Finally, out of curiosity, what's the most common names for restaurants and cafés? There are not that many surprises, here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'Tomasa', 'count': 3}\n",
      "{'_id': 'Que Pasa', 'count': 3}\n",
      "{'_id': 'Sadhu', 'count': 3}\n",
      "{'_id': 'Restaurant Split', 'count': 3}\n",
      "{'_id': 'Maria', 'count': 2}\n",
      "{'_id': 'Don Antonio', 'count': 2}\n",
      "{'_id': 'Fortuna', 'count': 2}\n",
      "{'_id': 'Steakhaus Barbecue', 'count': 2}\n",
      "{'_id': 'Roma', 'count': 2}\n",
      "{'_id': 'Cancún', 'count': 2}\n"
     ]
    }
   ],
   "source": [
    "# Common restaurant names in suburbs\n",
    "query = [\n",
    "        {'$match': {'amenity' : {'$eq' : 'restaurant'}, 'name': {'$exists': 'true'}}}, \n",
    "        {'$group': {'_id': '$name', 'count': {'$sum' : 1}}}, \n",
    "        {'$sort': {'count': -1}},\n",
    "        {'$limit': 10} \n",
    "]\n",
    "\n",
    "for doc in db.neukolln.aggregate(query):\n",
    "    pprint.pprint(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'Tchibo', 'count': 3}\n",
      "{'_id': 'Tele-Internetcafé', 'count': 2}\n",
      "{'_id': 'Thoben', 'count': 2}\n",
      "{'_id': 'Fräulein Frost', 'count': 2}\n",
      "{'_id': 'Café Obergfell', 'count': 2}\n",
      "{'_id': 'Brezel Company', 'count': 2}\n",
      "{'_id': 'Café Melanie', 'count': 2}\n",
      "{'_id': 'Eis Hennig', 'count': 2}\n",
      "{'_id': 'Marcello', 'count': 1}\n",
      "{'_id': 'soup2go', 'count': 1}\n"
     ]
    }
   ],
   "source": [
    "# Common cafe names in suburbs\n",
    "query = [\n",
    "        {'$match': {'amenity' : {'$eq' : 'cafe'}, 'name': {'$exists': 'true'}}}, \n",
    "        {'$group': {'_id': '$name', 'count': {'$sum' : 1}}}, \n",
    "        {'$sort': {'count': -1}},\n",
    "        {'$limit': 10} \n",
    "]\n",
    "\n",
    "for doc in db.neukolln.aggregate(query):\n",
    "    pprint.pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contribution of community to the map\n",
    "\n",
    "How many people have contributed to the map? And who has contributed most? Overall, a large numer of individuals have contributed. However, when looking at the top contributors (e.g. user [atpl_pilot](https://www.openstreetmap.org/user/atpl_pilot)) they have contributed tremendously and contributed a large share of the data (about 50% for useratpl_pilot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique contributors:  2152\n"
     ]
    }
   ],
   "source": [
    "# User Count\n",
    "user_count = db.neukolln.distinct('user')\n",
    "print(\"Number of unique contributors: \", len(user_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'atpl_pilot', 'count': 886084}\n",
      "{'_id': 'jacobbraeutigam', 'count': 183115}\n",
      "{'_id': 'anbr', 'count': 110080}\n",
      "{'_id': 'DonRudi', 'count': 62359}\n",
      "{'_id': 'Balgofil', 'count': 56431}\n",
      "{'_id': 'g0ldfish', 'count': 50273}\n",
      "{'_id': 'webpassenger', 'count': 36522}\n",
      "{'_id': 'geozeisig', 'count': 33281}\n",
      "{'_id': 'Elwood', 'count': 32581}\n",
      "{'_id': 'Polarbear', 'count': 22350}\n"
     ]
    }
   ],
   "source": [
    "# Top Ten Contributors\n",
    "query = [\n",
    "        {'$match': {'user' : {'$exists': 'true'}}}, \n",
    "        {'$group': {'_id': '$user', 'count': {'$sum' : 1}}}, \n",
    "        {'$sort': {'count': -1}},\n",
    "        {'$limit': 10} \n",
    "]\n",
    "\n",
    "for doc in db.neukolln.aggregate(query):\n",
    "    pprint.pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "During the analysis, I encountered the following minor problems while studying the data. Overall, data quality was very good and there were little noticable data-entry errors.\n",
    "\n",
    "*First*, the raw data includes a large share of lines of nodes that include information on changes in the data. While this data is relevant and great to have, in a next iteration of this analysis, I would write code to detect the information on the latest time of change and add only this information to the data-base. The change-set information especially distorts information on user contributions.\n",
    "\n",
    "*Second*, also, in a future iteration, it would be interesting to check which fields in the data-set, like surface information for streets, or wheelchair-access at buildings are still missing. It would be great to write code that would provide functionality for users to enter their area (lat, lon) and enter a topic that is important to them (like street surface for cyclists, wheelchair-friendly access for handycapped) and then getting a list of the 10 closest nodes that are missing this information. \n",
    "\n",
    "Implementation could be rather complex when working with the whole data-set. Thus, it would be necessary to limit the extract of the osm data that the user analyzes. For example, one could use a service like the overpass API to get the data. Next, one would need to create a python dictionary holding the information on which tags for specific types of nodes are possible. Then, one would check the osm data for the (non)presence of these tags and return to the user those nodes. The user would then have the possibility to contribute and provide information that is known to him/her. Another larger challenge would be to re-submit the data to the core osm project. The simplest solution from a programming perspective would be to encourage the user to go to the official osm site and add the information there. This would assure data integrity and allow for quality assurance. A solution that is easier for the user, i.e. one that would allow the automatic entry of data into the nodes returned to him/her, would be even more complex. One would have to programmatically push data to the osm core data-set, check data qualtiy, ensure that the data has not been changed since it was last retrieved, etc.\n",
    "\n",
    "Overall, I enjoyed working with data from the impressive osm data-set. With the tools above, I am now encouraged to use the data for future research. Especially in the social sciences, information about local businesses, densities of public institutions, traffic and other amenities can provide interesting insights into the social structures of cities and their political implications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
